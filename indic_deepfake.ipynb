{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":95302,"databundleVersionId":11325230,"sourceType":"competition"},{"sourceId":10916158,"sourceType":"datasetVersion","datasetId":6786106}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os, glob, random\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nimport timm\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Train and Test files","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/multilingual-indian-speech-data/metadata/train.csv')\ntest_df = pd.read_csv('/kaggle/input/multilingual-indian-speech-data/metadata/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['audio_path'] = '/kaggle/input/multilingual-indian-speech-data/audio/' + train_df['id'] + '.wav'\ntest_df['audio_path'] = '/kaggle/input/multilingual-indian-speech-data/audio/' + test_df['id'] + '.wav'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df[['id', 'language', 'is_tts','audio_path']].head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FakeVoiceDataset(Dataset):\n    def __init__(self, file_paths, labels, sr=16000, n_mels=128, duration=None, transform=None):\n        \"\"\"\n        Args:\n            file_paths (list): List of audio file paths.\n            labels (list): Corresponding labels (e.g., 0: real, 1: fake).\n            sr (int): Target sample rate.\n            n_mels (int): Number of Mel bands.\n            duration (int or None): If set, audio is padded/truncated to this many seconds.\n                                    If None, the entire audio file is used.\n            transform: Optional transform to be applied on the Mel spectrogram.\n        \"\"\"\n        self.file_paths = file_paths\n        self.labels = labels\n        self.sr = sr\n        self.n_mels = n_mels\n        self.duration = duration\n        self.transform = transform\n        \n        if self.duration is not None:\n            self.samples = int(sr * duration)\n        else:\n            self.samples = None\n\n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        file_path = self.file_paths[idx]\n        y, _ = librosa.load(file_path, sr=self.sr)\n        \n        if self.samples is not None:\n            if len(y) < self.samples:\n                y = np.pad(y, (0, self.samples - len(y)), mode='constant')\n            else:\n                y = y[:self.samples]\n        \n        mel_spec = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n        \n        mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min())\n        \n        mel_spec = np.stack([mel_spec, mel_spec, mel_spec], axis=0)\n        \n        mel_spec = torch.tensor(mel_spec, dtype=torch.float)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        \n        if self.transform:\n            mel_spec = self.transform(mel_spec)\n            \n        return mel_spec, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_paths = train_df['audio_path'].tolist()\nlabels = train_df['is_tts'].tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_transform = transforms.Compose([\n    transforms.Resize((224, 224))\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = FakeVoiceDataset(file_paths, labels, sr=16000, n_mels=128, duration=None, transform=resize_transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_samples = len(dataset)\nnum_train = int(0.7 * num_samples)\nnum_val = num_samples - num_train\ntrain_dataset, val_dataset = random_split(dataset, [num_train, num_val])\n\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 10 \n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    epoch_loss = running_loss / num_train\n    \n    # Validation\n    model.eval()\n    all_targets = []\n    all_preds = []\n    with torch.no_grad():\n        for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            probabilities = torch.softmax(outputs, dim=1)[:, 1]\n            all_targets.extend(targets.cpu().numpy())\n            all_preds.extend(probabilities.cpu().numpy())\n    auc = roc_auc_score(all_targets, all_preds)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val AUC-ROC: {auc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestVoiceDataset(Dataset):\n    def __init__(self, df, sr=16000, n_mels=128, duration=3, transform=None):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): DataFrame containing at least 'id' and 'audio_path' columns.\n            sr (int): Sample rate.\n            n_mels (int): Number of mel bands.\n            duration (int or None): Fixed duration (in seconds) to pad/truncate audio. Set to None to use full audio.\n            transform: Optional transform to apply on the mel spectrogram.\n        \"\"\"\n        self.df = df\n        self.sr = sr\n        self.n_mels = n_mels\n        self.duration = duration\n        self.transform = transform\n        if self.duration is not None:\n            self.samples = int(sr * duration)\n        else:\n            self.samples = None\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        audio_path = row['audio_path']\n        sample_id = row['id']\n        \n        # Load audio\n        y, _ = librosa.load(audio_path, sr=self.sr)\n        \n        # Pad or truncate audio if a fixed duration is specified\n        if self.samples is not None:\n            if len(y) < self.samples:\n                y = np.pad(y, (0, self.samples - len(y)), mode='constant')\n            else:\n                y = y[:self.samples]\n        \n        # Compute Mel spectrogram\n        mel_spec = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n        \n        # Normalize to [0, 1]\n        mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min())\n        \n        # Stack to create 3 channels\n        mel_spec = np.stack([mel_spec, mel_spec, mel_spec], axis=0)  # Shape: (3, n_mels, time)\n        mel_spec = torch.tensor(mel_spec, dtype=torch.float)\n        \n        if self.transform:\n            mel_spec = self.transform(mel_spec)\n        \n        return mel_spec, sample_id\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"fake_voice_vit.pth\")\n\nclass TestVoiceDataset(Dataset):\n    def __init__(self, df, sr=16000, n_mels=128, duration=3, transform=None):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): DataFrame containing at least 'id' and 'audio_path' columns.\n            sr (int): Sample rate.\n            n_mels (int): Number of mel bands.\n            duration (int or None): Fixed duration (in seconds) to pad/truncate audio. Set to None to use full audio.\n            transform: Optional transform to apply on the mel spectrogram.\n        \"\"\"\n        self.df = df\n        self.sr = sr\n        self.n_mels = n_mels\n        self.duration = duration\n        self.transform = transform\n        if self.duration is not None:\n            self.samples = int(sr * duration)\n        else:\n            self.samples = None\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        audio_path = row['audio_path']\n        sample_id = row['id']\n        \n        # Load audio\n        y, _ = librosa.load(audio_path, sr=self.sr)\n        \n        # Pad or truncate audio if a fixed duration is specified\n        if self.samples is not None:\n            if len(y) < self.samples:\n                y = np.pad(y, (0, self.samples - len(y)), mode='constant')\n            else:\n                y = y[:self.samples]\n        \n        # Compute Mel spectrogram\n        mel_spec = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)\n        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n        \n        # Normalize to [0, 1]\n        mel_spec = (mel_spec - mel_spec.min()) / (mel_spec.max() - mel_spec.min())\n        \n        # Stack to create 3 channels\n        mel_spec = np.stack([mel_spec, mel_spec, mel_spec], axis=0)  # Shape: (3, n_mels, time)\n        mel_spec = torch.tensor(mel_spec, dtype=torch.float)\n        \n        if self.transform:\n            mel_spec = self.transform(mel_spec)\n        \n        return mel_spec, sample_id\n\n\n# test_csv_path = '/kaggle/input/multilingual-indian-speech-data/metadata/test.csv'\n# test_df = pd.read_csv(test_csv_path)\n\ntest_dataset = TestVoiceDataset(test_df, sr=16000, n_mels=128, duration=None, transform=resize_transform)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n\nall_ids = []\nall_preds = []\n\nwith torch.no_grad():\n    for inputs, ids in tqdm(test_loader, desc=\"Inference\"):\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        # For binary classification, we take the probability of class 1 (is_tts)\n        probs = torch.softmax(outputs, dim=1)[:, 1]\n        all_preds.extend(probs.cpu().numpy())\n        all_ids.extend(ids)\n\n# ------------------------------------------\n# Create Submission DataFrame and Save to CSV\n# ------------------------------------------\nsubmission_df = pd.DataFrame({'id': all_ids, 'is_tts': all_preds})\n# Optionally sort submission_df by id if needed:\nsubmission_df = submission_df.sort_values('id')\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"Inference complete and submission.csvÂ saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}